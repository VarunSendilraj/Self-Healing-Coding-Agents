"""
Simple, direct fitness evaluator for evolutionary prompt optimization.
Just tests the actual planner-executor output against the original test cases.
"""

import logging
import time
from typing import List, Dict, Any, Optional, Tuple

logger = logging.getLogger(__name__)


class SimpleFitnessEvaluator:
    """
    Simple fitness evaluator that directly tests planner-executor pipeline output
    against the original test cases from the critic. No complex scoring.
    """
    
    def __init__(self, planner_agent, executor_agent, critic_agent, task_description: str, original_test_cases: List[Dict[str, Any]]):
        """
        Initialize with the actual agents and test cases.
        
        Args:
            planner_agent: The planner agent
            executor_agent: The executor agent  
            critic_agent: The critic agent (for test execution)
            task_description: The original task description
            original_test_cases: Test cases generated by the critic for the original task
        """
        self.planner_agent = planner_agent
        self.executor_agent = executor_agent
        self.critic_agent = critic_agent
        self.task_description = task_description
        self.original_test_cases = original_test_cases
        
        # Cache for fitness evaluations
        self.evaluation_cache = {}
        self.evaluation_count = 0
        
        logger.info(f"ðŸŽ¯ SIMPLE FITNESS: Initialized with {len(original_test_cases)} test cases")
        for i, test_case in enumerate(original_test_cases[:3], 1):  # Log first 3 test cases
            inputs = test_case.get('inputs', {})
            expected = test_case.get('expected_output', 'unknown')
            logger.info(f"   Test {i}: {inputs} â†’ {expected}")
    
    def evaluate_prompt(self, prompt: str, agent_type: str) -> float:
        """
        Evaluate a prompt by testing the planner-executor pipeline output against test cases.
        
        Args:
            prompt: The prompt to evaluate
            agent_type: Either "PLANNER" or "EXECUTOR"
            
        Returns:
            Fitness score (0.0 - 1.0) based on test case pass rate
        """
        # Check cache first
        cache_key = f"{agent_type}:{hash(prompt)}"
        if cache_key in self.evaluation_cache:
            logger.debug(f"ðŸ”„ CACHE HIT: Using cached fitness for {agent_type} prompt")
            return self.evaluation_cache[cache_key]
        
        logger.info(f"ðŸ§ª EVALUATING: {agent_type} prompt fitness")
        start_time = time.time()
        
        try:
            # Store original prompts
            original_planner_prompt = self.planner_agent.system_prompt
            original_executor_prompt = self.executor_agent.system_prompt
            
            # Set the evolved prompt
            if agent_type == "PLANNER":
                self.planner_agent.system_prompt = prompt
            else:  # EXECUTOR
                self.executor_agent.system_prompt = prompt
            
            # Run the planner-executor pipeline
            logger.debug(f"   ðŸ”„ Running planner...")
            plan = self.planner_agent.run(user_request=self.task_description)
            
            if isinstance(plan, dict) and plan.get("error"):
                logger.warning(f"   âŒ Planner failed: {plan.get('error')}")
                return 0.0
            
            logger.debug(f"   ðŸ”„ Running executor...")
            code = self.executor_agent.run(plan=plan, original_request=self.task_description)
            
            if isinstance(code, dict) and code.get("error"):
                logger.warning(f"   âŒ Executor failed: {code.get('error')}")
                return 0.1  # Small score for getting past planner
            
            # Test the generated code against original test cases
            logger.debug(f"   ðŸ§ª Testing code against {len(self.original_test_cases)} test cases...")
            test_results = self._test_code_directly(code)
            
            # Calculate fitness based on test results
            if test_results["total_tests"] == 0:
                fitness = 0.0
            else:
                fitness = test_results["passed_tests"] / test_results["total_tests"]
            
            # Small bonus for no syntax/runtime errors
            if test_results["syntax_errors"] == 0 and test_results["runtime_errors"] == 0:
                fitness += 0.05
            
            # Cap at 1.0
            fitness = min(1.0, fitness)
            
            execution_time = time.time() - start_time
            logger.info(f"   âœ… FITNESS: {fitness:.3f} ({test_results['passed_tests']}/{test_results['total_tests']} tests passed) in {execution_time:.1f}s")
            
            # Cache result
            self.evaluation_cache[cache_key] = fitness
            self.evaluation_count += 1
            
            return fitness
            
        except Exception as e:
            logger.error(f"   âŒ FITNESS EVALUATION ERROR: {e}")
            return 0.0
            
        finally:
            # Restore original prompts
            self.planner_agent.system_prompt = original_planner_prompt
            self.executor_agent.system_prompt = original_executor_prompt
    
    def batch_evaluate(self, prompts: List[str], agent_type: str) -> List[float]:
        """
        Evaluate multiple prompts.
        
        Args:
            prompts: List of prompts to evaluate
            agent_type: Either "PLANNER" or "EXECUTOR"
            
        Returns:
            List of fitness scores
        """
        logger.info(f"ðŸ“Š BATCH EVAL: Evaluating {len(prompts)} {agent_type} prompts")
        
        scores = []
        for i, prompt in enumerate(prompts, 1):
            logger.debug(f"   Evaluating prompt {i}/{len(prompts)}")
            score = self.evaluate_prompt(prompt, agent_type)
            scores.append(score)
        
        logger.info(f"   âœ… BATCH COMPLETE: Scores range {min(scores):.3f} - {max(scores):.3f}")
        return scores
    
    def _test_code_directly(self, code: str) -> Dict[str, int]:
        """
        Test the generated code directly against the original test cases.
        
        Args:
            code: The generated code to test
            
        Returns:
            Dictionary with test results
        """
        results = {
            "total_tests": len(self.original_test_cases),
            "passed_tests": 0,
            "failed_tests": 0,
            "syntax_errors": 0,
            "runtime_errors": 0
        }
        
        if not self.original_test_cases:
            logger.warning("   âš ï¸ No test cases available for evaluation")
            return results
        
        try:
            # Extract the main function from the code
            function_name = self._extract_function_name(code)
            if not function_name:
                logger.warning(f"   âš ï¸ Could not identify function to test in code")
                results["syntax_errors"] = 1
                return results
            
            # Create a safe execution environment
            exec_globals = {
                "__builtins__": {
                    "len": len,
                    "str": str,
                    "int": int,
                    "float": float,
                    "bool": bool,
                    "list": list,
                    "dict": dict,
                    "set": set,
                    "tuple": tuple,
                    "range": range,
                    "enumerate": enumerate,
                    "min": min,
                    "max": max,
                    "sum": sum,
                    "abs": abs,
                    "print": print,  # Allow print for debugging
                }
            }
            exec_locals = {}
            
            # Execute the code to define the function
            exec(code, exec_globals, exec_locals)
            
            # Get the function
            if function_name not in exec_locals:
                logger.warning(f"   âš ï¸ Function '{function_name}' not found in executed code")
                results["syntax_errors"] = 1
                return results
            
            test_function = exec_locals[function_name]
            
            # Test each case
            for i, test_case in enumerate(self.original_test_cases):
                try:
                    inputs = test_case.get("inputs", {})
                    expected = test_case.get("expected_output")
                    test_name = test_case.get("test_case_name", f"test_{i}")
                    
                    # Call the function with inputs
                    if isinstance(inputs, dict):
                        # Handle keyword arguments
                        actual = test_function(**inputs)
                    elif isinstance(inputs, (list, tuple)):
                        # Handle positional arguments
                        actual = test_function(*inputs)
                    else:
                        # Single argument
                        actual = test_function(inputs)
                    
                    # Convert actual to same type as expected for comparison
                    if isinstance(expected, bool):
                        actual = bool(actual)
                    elif isinstance(expected, int):
                        actual = int(actual) if actual is not None else None
                    elif isinstance(expected, str):
                        actual = str(actual) if actual is not None else None
                    
                    if actual == expected:
                        results["passed_tests"] += 1
                        logger.debug(f"   âœ… {test_name}: {inputs} â†’ {actual}")
                    else:
                        results["failed_tests"] += 1
                        logger.debug(f"   âŒ {test_name}: {inputs} â†’ {actual}, expected {expected}")
                        
                except Exception as e:
                    results["runtime_errors"] += 1
                    logger.debug(f"   âŒ Runtime error on {test_case.get('test_case_name', f'test_{i}')}: {e}")
            
        except SyntaxError as e:
            results["syntax_errors"] = 1
            logger.debug(f"   âŒ Syntax error in generated code: {e}")
        except Exception as e:
            results["runtime_errors"] = 1
            logger.debug(f"   âŒ Execution error: {e}")
        
        return results
    
    def _extract_function_name(self, code: str) -> Optional[str]:
        """
        Extract the main function name from the code.
        
        Args:
            code: The generated code
            
        Returns:
            Function name or None if not found
        """
        import re
        
        # Look for function definitions
        function_pattern = r'def\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*\('
        matches = re.findall(function_pattern, code)
        
        if matches:
            # Prioritize common function names for specific problems
            common_names = ['isMatch', 'is_match', 'match', 'solve', 'solution', 'main']
            for name in common_names:
                if name in matches:
                    return name
            
            # Return the first function found
            return matches[0]
        
        return None
    
    def get_stats(self) -> Dict[str, Any]:
        """Get evaluation statistics."""
        return {
            "total_evaluations": self.evaluation_count,
            "cache_size": len(self.evaluation_cache),
            "test_cases_count": len(self.original_test_cases)
        } 