# Evolutionary Prompt Optimization for Self-Healing Multi-Agent Systems

This package implements a sophisticated evolutionary algorithm approach to systematically optimize prompts for multi-agent systems based on failure analysis and fitness evaluation.

## üß¨ Overview

The evolutionary prompt optimization system addresses a key limitation in current self-healing approaches: **single-shot LLM prompt modifications often get trapped in local optima**. Our solution uses population-based evolutionary algorithms to systematically explore the prompt space for improved self-healing success rates.

### Key Innovation
- **LLM-guided Evolution**: Uses LLMs themselves as evolutionary operators (crossover/mutation) while maintaining population-based search dynamics
- **Multi-objective Fitness**: Evaluates prompts on success rate, efficiency, coherence, and generalization
- **Semantic Preservation**: Ensures evolved prompts maintain agent functionality while improving performance

## üèóÔ∏è Architecture

### Core Components

```
EvolutionaryPromptOptimizer
‚îú‚îÄ‚îÄ PromptPopulation          # Population management & selection
‚îú‚îÄ‚îÄ PromptFitnessEvaluator   # Multi-objective fitness evaluation  
‚îú‚îÄ‚îÄ EvolutionOperators       # LLM-guided crossover & mutation
‚îî‚îÄ‚îÄ EvolutionConfig          # Hyperparameter configuration
```

### Workflow

1. **Population Initialization**: Generate diverse prompts using different strategies
2. **Fitness Evaluation**: Assess prompts on validation tasks
3. **Selection**: Roulette wheel + elite preservation
4. **Evolution**: LLM-guided crossover and mutation
5. **Population Update**: Merge offspring and select survivors
6. **Convergence**: Terminate on improvement plateau or target fitness

## üöÄ Quick Start

### Basic Usage

```python
from self_healing_agents.evolution import create_executor_optimizer
from self_healing_agents.llm_service import LLMService

# Initialize LLM service
llm_service = LLMService(provider="openai", model_name="gpt-4")

# Create optimizer
optimizer = create_executor_optimizer(llm_service)

# Optimize a prompt
results = optimizer.optimize_prompt(
    base_prompt="You are a Python programmer. Write code.",
    agent_type="EXECUTOR",
    failure_context={"task": "implement sorting algorithm"}
)

print(f"Best fitness: {results.best_fitness:.3f}")
print(f"Evolved prompt: {results.best_prompt}")
```

### Integration with Self-Healing

```python
from self_healing_agents.evaluation.evolutionary_enhanced_harness import (
    run_evolutionary_multi_agent_task
)

# Run task with evolutionary self-healing
result = run_evolutionary_multi_agent_task(
    task_definition=your_task,
    planner=planner_agent,
    executor=executor_agent,
    critic=critic_agent,
    llm_service_instance=llm_service,
    use_evolutionary_optimization=True
)
```

## ‚öôÔ∏è Configuration

### Evolution Parameters

```python
from self_healing_agents.evolution import EvolutionConfig

# Standard configuration
config = EvolutionConfig(
    population_size=5,
    max_generations=8,
    crossover_rate=0.8,
    mutation_rate=0.3,
    max_evaluations=50
)

# Optimized for planners
planner_config = EvolutionConfig.for_planner()

# Fast mode for testing
fast_config = EvolutionConfig.fast_mode()
```

### Fitness Function Weights

```python
config = EvolutionConfig(
    success_rate_weight=0.5,    # Task completion success
    efficiency_weight=0.2,      # Speed and token efficiency  
    coherence_weight=0.2,       # Prompt semantic quality
    generalization_weight=0.1   # Performance across tasks
)
```

## üß™ Evolution Operators

### Population Initialization
- **Instruction-focused**: Enhanced directives and clarity
- **Example-enhanced**: More demonstrations and samples
- **Constraint-focused**: Explicit limitations and requirements
- **Role-definition**: Strengthened agent identity
- **Hybrid**: Combined approach strategies

### Crossover (Semantic Recombination)
1. **Component Extraction**: Identify role, instructions, examples, constraints
2. **LLM-guided Combination**: Intelligently merge best elements
3. **Coherence Validation**: Ensure semantic consistency

### Mutation (Controlled Variation)
- **Instruction Refinement** (30%): Clearer, more specific directives
- **Example Enhancement** (25%): Improved demonstrations
- **Structure Optimization** (25%): Better organization and flow
- **Role Enhancement** (20%): Strengthened agent identity

## üìä Fitness Evaluation

### Multi-Objective Assessment

**F(prompt) = Œ±√ósuccess_rate + Œ≤√óefficiency + Œ≥√ócoherence + Œ¥√ógeneralization**

Where:
- **Success Rate**: Task completion percentage on validation set
- **Efficiency**: Inverse of execution time and token count
- **Coherence**: Semantic quality via LLM evaluation
- **Generalization**: Consistency across different tasks

### Validation Tasks
- Default: 3 representative coding/planning tasks
- Customizable: Define domain-specific validation sets
- Cached: Results cached to avoid redundant evaluations

## üéØ Performance

### Expected Improvements
- **15-25% better healing success rate** vs current LLM-based healing
- **10-20% higher solution quality** (final task scores)
- **Convergence within 5-8 generations** for most tasks

### Computational Efficiency
- **Budget Management**: Hard limits on LLM calls (default: 50)
- **Early Stopping**: Terminates on convergence (3 generations no improvement)
- **Caching**: Fitness results cached to avoid re-evaluation

## üîß Advanced Features

### Adaptive Mechanisms
- **Adaptive Mutation**: Increases rate when diversity drops
- **Diversity Maintenance**: Penalties for overly similar prompts
- **Elite Preservation**: Top 20% automatically survive

### Quality Assurance
- **Semantic Validation**: Each generation checked for coherence
- **Rollback Mechanism**: Revert if quality degrades
- **Coherence Scoring**: LLM-based validation of prompt quality

### Monitoring & Logging
- **Generation Statistics**: Fitness trends, diversity metrics
- **Evolution History**: Complete trace of optimization process
- **Performance Analytics**: Detailed timing and efficiency metrics

## üìà Evaluation Metrics

### Primary Metrics
- **Healing Success Rate**: % of failures resolved
- **Solution Quality**: Final task performance scores
- **Convergence Efficiency**: Generations to optimal solution

### Secondary Metrics  
- **Computational Cost**: Total LLM calls and execution time
- **Prompt Quality**: Human evaluation of evolved prompts
- **Generalization**: Performance on unseen tasks

## üß™ Testing

### Run the Test Suite

```bash
# From the src directory
python self_healing_agents/test_evolutionary_system.py
```

### Component Testing

```python
# Test individual components
from self_healing_agents.evolution import EvolutionaryPromptOptimizer

optimizer = EvolutionaryPromptOptimizer(llm_service)
results = optimizer.optimize_prompt("Your prompt", "EXECUTOR")
```

### Full Workflow Testing

```python
# Test complete self-healing workflow
from self_healing_agents.evaluation.evolutionary_enhanced_harness import (
    test_evolutionary_healing_with_real_llm
)

results = test_evolutionary_healing_with_real_llm()
```

## üî¨ Research Applications

### Ablation Studies
- Population size effects: N = {3, 5, 8, 10}
- Selection strategies: Roulette vs Tournament vs Elite
- Operator rates: Crossover {0.6, 0.8, 1.0}, Mutation {0.1, 0.3, 0.5}
- Fitness component weights impact

### Experimental Design
- **Baseline Comparisons**: Current LLM healing, random sampling, manual engineering
- **Statistical Validation**: A/B testing across 50+ task instances
- **Domain Generalization**: Multiple task categories and agent types

## üìö API Reference

### Main Classes

- **`EvolutionaryPromptOptimizer`**: Main orchestrator for evolutionary optimization
- **`PromptPopulation`**: Population management and selection operators
- **`PromptFitnessEvaluator`**: Multi-objective fitness evaluation
- **`EvolutionOperators`**: LLM-guided crossover and mutation
- **`EvolutionConfig`**: Configuration and hyperparameters

### Factory Functions

- **`create_planner_optimizer(llm_service)`**: Optimizer for planner prompts
- **`create_executor_optimizer(llm_service)`**: Optimizer for executor prompts  
- **`create_fast_optimizer(llm_service)`**: Quick optimization for testing

## ü§ù Contributing

1. Follow the modular architecture
2. Add comprehensive tests for new components
3. Update documentation for API changes
4. Run the full test suite before submitting
5. Include ablation studies for new hyperparameters

## üìÑ License

See the main project license for details.

---

## üéì Research Context

This implementation is based on recent advances in:
- **Evolutionary Prompt Engineering** (Zhou et al., 2023)
- **Multi-Agent Self-Healing Systems** (Various, 2023-2024)  
- **LLM-guided Optimization** (Emerging research area)

The key insight is treating prompts as evolvable populations rather than fixed instructions, enabling systematic exploration of the prompt space for improved performance. 