from dataclasses import dataclass, field
from typing import List, Tuple, Optional, Any, Dict # Any for now, will be more specific later

from self_healing_agents.schemas import (
    CriticReport,
    CRITIC_STATUS_SUCCESS,
    CRITIC_STATUS_FAILURE_SYNTAX,
    CRITIC_STATUS_FAILURE_RUNTIME,
    CRITIC_STATUS_FAILURE_LOGIC
)

from self_healing_agents.llm_service import LLMService, LLMServiceError

# Placeholder for Orchestrator functions if needed for sub-loops (Task 3.6)
# from self_healing_agents.orchestrator import ... 

MAX_POPULATION_SIZE_N = 3 # As per PRD (e.g., N=2 or 3)
EVO_PROMPT_ITERATIONS_T = 1 # As per PRD (e.g., T=1 or T=2)

@dataclass
class PromptInfo:
    prompt: str
    score: float
    # Optional: age/iteration_created to help with tie-breaking or other strategies
    iteration_created: int = 0 
    # Optional: store the critic report that led to this score for more detailed evolution
    critic_report: Optional[CriticReport] = None 

class PromptModifier:
    """
    Manages the EvoPrompt process for a single task instance to refine prompts for an Executor agent.
    This class is stateful per task instance.
    """
    def __init__(self, llm_service: LLMService, initial_prompt: str, initial_score: float, initial_critic_report: CriticReport, task_id: str, max_population_size: int = MAX_POPULATION_SIZE_N):
        """
        Initializes the PromptModifier for a new task instance.

        Args:
            llm_service: An instance of LLMService to use for prompt evolution.
            initial_prompt: The first failing prompt from the Executor.
            initial_score: The score of the code generated by the initial_prompt.
            initial_critic_report: The critic report for the initial prompt's output.
            task_id: A unique identifier for the current task instance.
            max_population_size: The maximum number of prompts to maintain in the population.
        """
        self.llm_service = llm_service
        self.task_id = task_id
        self.max_population_size = max_population_size
        self.prompt_population: List[PromptInfo] = []
        self.current_evo_iteration = 0 # Internal EvoPrompt iterations (t=1..T)
        self.main_system_healing_attempts = 0 # Tracks how many times this modifier has been called in the main loop

        # Initialize population (Task 3.3 will refine this)
        if initial_prompt is not None and initial_score is not None:
             self._add_to_population(PromptInfo(prompt=initial_prompt, score=initial_score, iteration_created=0, critic_report=initial_critic_report))

    def _add_to_population(self, prompt_info: PromptInfo):
        """Adds a prompt to the population, sorts by score (and then by recency for ties), and prunes to maintain max_population_size."""
        self.prompt_population.append(prompt_info)
        # Sort by score (descending), then by iteration_created (descending) to favor newer prompts in case of a tie in scores.
        self.prompt_population.sort(key=lambda pi: (pi.score, pi.iteration_created), reverse=True)
        if len(self.prompt_population) > self.max_population_size:
            self.prompt_population = self.prompt_population[:self.max_population_size]

    def get_current_population(self) -> List[PromptInfo]:
        """Returns the current prompt population."""
        return self.prompt_population

    def select_parents(self, failing_prompt_info_for_current_iteration: PromptInfo) -> List[PromptInfo]:
        """
        Selects parent prompts for evolution based on the MVP strategy (PRD Task 3.4).
        
        Args:
            failing_prompt_info_for_current_iteration: The PromptInfo for the prompt that just failed and triggered healing.
            
        Returns:
            A list of PromptInfo objects to be used as parents for evolution (1 or 2 prompts).
        """
        selected_parents_map: Dict[str, PromptInfo] = {}

        if not self.prompt_population:
            # This case should ideally be avoided if run_self_healing_iteration ensures population before calling.
            # However, if called and population is empty, and failing_prompt_info is valid, use it.
            if failing_prompt_info_for_current_iteration:
                 selected_parents_map[failing_prompt_info_for_current_iteration.prompt] = failing_prompt_info_for_current_iteration
            return list(selected_parents_map.values())

        # Always include the current failing prompt as a primary parent candidate.
        # Its critic_report (obtained separately in run_self_healing_iteration) is crucial for guiding evolution.
        selected_parents_map[failing_prompt_info_for_current_iteration.prompt] = failing_prompt_info_for_current_iteration

        # If not the first healing attempt for this task instance (i.e., main_system_healing_attempts > 1),
        # and the population offers other options, also consider the historically best prompt if it's different.
        if self.main_system_healing_attempts > 1:
            historically_best_prompt_in_population = self.prompt_population[0] # Population is sorted by score desc
            
            # Add if different from the failing_prompt_info already added
            if historically_best_prompt_in_population.prompt not in selected_parents_map:
                selected_parents_map[historically_best_prompt_in_population.prompt] = historically_best_prompt_in_population
        
        return list(selected_parents_map.values())

    def evolve_prompts(self, parents: List[PromptInfo], critic_feedback: CriticReport) -> List[str]:
        """Generates new candidate prompts using LLM-based evolution. Placeholder for Task 3.5."""
        if not parents:
            return []

        evolved_prompts: List[str] = []
        
        # For MVP, let's assume we primarily evolve based on the first parent, 
        # which is typically the most recent failing prompt.
        # The critic_feedback directly corresponds to this first parent.
        primary_parent_info = parents[0]

        system_message = {
            "role": "system",
            "content": "You are an expert prompt engineer specializing in crafting system prompts for Python code generation agents. Your goal is to evolve a given system prompt to make the agent perform better, based on specific failure feedback. Output only the new, complete system prompt, and nothing else. Do not include explanations or apologies."
        }

        # Constructing a detailed user message for the LLM
        # Incorporating PRD's examples of holistic prompt evolution and failure details.
        # Example of holistic evolution from PRD:
        # Original: "You are a meticulous Python programmer. Write code according to the spec. Output only code."
        # Evolved: "You are a software engineer writing production-ready Python code. Avoid relying on high-level shortcuts; focus on correctness and edge-case handling."
        # Evolved further: "You are a senior software engineer writing production-ready Python code that is explicit, handles inputs defensively, and includes minimal inline testing to ensure correctness. Avoid relying on high-level shortcuts; Return only valid Python code."

        failure_details = f"Status: {critic_feedback.status}\\nScore: {critic_feedback.score:.2f}\\nSummary: {critic_feedback.summary}"
        if critic_feedback.error_details:
            failure_details += f"\\nError Details: {critic_feedback.error_details}"
        if critic_feedback.test_results:
            failed_tests_summary = "\\nFailed Test Cases:\\n"
            for tc_res in critic_feedback.test_results:
                # Assuming tc_res is a dict with at least these keys if not passed
                # This structure depends on how test_results are actually formatted by the Critic
                # For now, let's check for common keys to avoid KeyErrors if tc_res is minimal
                tc_name = tc_res.get("test_case_name", tc_res.get("name", "Unnamed Test"))
                tc_inputs = tc_res.get("inputs", "N/A")
                tc_expected = tc_res.get("expected_output", tc_res.get("expected", "N/A"))
                tc_actual = tc_res.get("actual_output", tc_res.get("actual", "N/A"))
                tc_error = tc_res.get("error_message", "N/A")
                tc_passed = tc_res.get("passed", False) # Default to False if 'passed' key is missing

                if not tc_passed:
                    failed_tests_summary += f"- Name: {tc_name}, Input: {tc_inputs}, Expected: {tc_expected}, Actual: {tc_actual}, Error: {tc_error}\\n"
            if failed_tests_summary.strip() != "Failed Test Cases:": # Add only if there are actual failed tests
                 failure_details += failed_tests_summary
        
        # For now, let's generate ONE evolved prompt based on the primary parent and its direct feedback.
        # The PRD mentions generating "1-2 new candidate holistic system prompts". 
        # We can extend this to iterate or use multiple parents more directly later.
        user_message_content = (
            f"The following system prompt for a Python code generation agent resulted in failures.\n"
            f"Original System Prompt:\n'''\n{primary_parent_info.prompt}\n'''\n\n"
            f"Failure Feedback (score {primary_parent_info.score:.2f}):\n{failure_details}\n\n"
            f"Instructions: Generate a new, complete, and holistically improved system prompt for the Python Executor agent. "
            f"The new prompt must guide the agent to avoid the previous errors and improve its performance on the task. "
            f"Ensure the new prompt embodies the following principles for the agent:\n"
            f"1. Emphasize disciplined problem decomposition and strong algorithmic thinking.\n"
            f"2. Mandate comprehensive edge case analysis and explicit handling in the generated code.\n"
            f"3. Instruct the agent to select and use the most appropriate data structures for the problem.\n"
            f"4. Reinforce the need for code that is not only correct but also efficient and readable.\n"
            f"5. Guide the agent towards defensive programming practices and robust input validation.\n"
            f"6. The agent should be reminded to consider time and space complexity implications.\n"
            f"Do NOT just patch the specific error mentioned in the feedback; aim for a generally better and more robust guiding prompt for the agent. "
            f"The new prompt should be a complete replacement of the original. "
            f"Return only the raw text of the new system prompt."
        )
        
        messages = [
            system_message,
            {"role": "user", "content": user_message_content}
        ]

        try:
            print(f"  [PromptModifier - {self.task_id}] Calling LLM to evolve prompt based on parent (score: {primary_parent_info.score:.2f}). Parent: \\\"{primary_parent_info.prompt[:70]}...\\\"")
            # Assuming LLMService.invoke returns a string directly for now.
            # The PRD says "generate 1-2 new candidate... prompts". If multiple are returned, parsing will be needed.
            # For simplicity, we'll assume one prompt string is returned.
            new_prompt_text = self.llm_service.invoke(messages, expect_json=False)
            
            if isinstance(new_prompt_text, str) and new_prompt_text.strip():
                # Basic cleaning: strip whitespace and potentially markdown code fences if LLM adds them
                cleaned_prompt = new_prompt_text.strip()
                if cleaned_prompt.startswith("```") and cleaned_prompt.endswith("```"):
                    # Remove the fences, find the first newline and take everything after
                    cleaned_prompt = cleaned_prompt.split('\\n', 1)[1 if '\\n' in cleaned_prompt else 0]
                    cleaned_prompt = cleaned_prompt.rsplit('\\n', 1)[0] if '\\n' in cleaned_prompt and cleaned_prompt.endswith("```") else cleaned_prompt
                    if cleaned_prompt.endswith("```"): # If it was ```python ... ```
                         cleaned_prompt = cleaned_prompt[:-3].strip()
                
                # Further check if the LLM starts with "New System Prompt:" or similar, and try to remove it.
                common_prefixes = ["New System Prompt:", "Evolved System Prompt:", "Here is the new system prompt:"]
                for prefix in common_prefixes:
                    if cleaned_prompt.lower().startswith(prefix.lower()):
                        cleaned_prompt = cleaned_prompt[len(prefix):].lstrip()
                        break # Found and removed a prefix

                if cleaned_prompt: # Ensure not empty after cleaning
                    evolved_prompts.append(cleaned_prompt)
                    print(f"    LLM generated evolved prompt: \\\"{cleaned_prompt[:70]}...\\\"")
                else:
                    print(f"    LLM generated an empty or poorly formatted prompt after cleaning. Original response: \\\"{new_prompt_text[:100]}...\\\"")
            else:
                print(f"    LLM did not return a valid string prompt. Response: {new_prompt_text}")

        except LLMServiceError as e:
            print(f"    Error during LLM-based prompt evolution: {e}")
        except Exception as e:
            # Catch any other unexpected errors during the LLM call or processing
            print(f"    Unexpected error during prompt evolution: {e}")
            
        # If the LLM was tasked to produce 2 prompts, the logic here would need to handle that.
        # For now, it generates one based on the primary parent.
        return evolved_prompts

    def evaluate_candidate_prompt(self, candidate_prompt: str, original_task_description: Any, current_critic_report_for_candidate: CriticReport) -> PromptInfo:
        """Processes the CriticReport for a candidate prompt to create a PromptInfo object."""
        # The actual execution of Executor -> Critic to get the current_critic_report_for_candidate
        # is handled by the orchestrator_callback_evaluate_candidate passed to run_self_healing_iteration.
        # This method's role is to package the results into a PromptInfo object.
        print(f"[PromptModifier - {self.task_id}] Processing evaluation results for candidate prompt: {candidate_prompt[:50]}... Score: {current_critic_report_for_candidate.score:.2f}")
        
        return PromptInfo(
            prompt=candidate_prompt, 
            score=current_critic_report_for_candidate.score, 
            iteration_created=self.current_evo_iteration + 1, # current_evo_iteration is (t+1) from run_self_healing_iteration
            critic_report=current_critic_report_for_candidate
        )

    def run_self_healing_iteration(self, failing_prompt_info: PromptInfo, original_task_description: Any, orchestrator_callback_evaluate_candidate: callable) -> Optional[str]:
        """
        Runs one full iteration of the constrained EvoPrompt process (T internal iterations).
        This is the main entry point for the orchestrator to call when self-healing is triggered.

        Args:
            failing_prompt_info: The PromptInfo object for the prompt that just failed.
            original_task_description: The original task description given to the Planner.
                                       This is needed if the Executor needs it alongside the prompt.
            orchestrator_callback_evaluate_candidate: A callable function.
                Expected signature: evaluate(candidate_prompt_text: str, task_description: Any) -> CriticReport
                This function is responsible for taking a candidate prompt string,
                giving it to the Executor, getting new code, evaluating that code with the Critic,
                and returning the full CriticReport.

        Returns:
            The best prompt found after T EvoPrompt iterations, or None if no better prompt is found or an error occurs.
        """
        self.main_system_healing_attempts += 1
        print(f"[PromptModifier - {self.task_id}] Starting self-healing attempt #{self.main_system_healing_attempts} for prompt (score: {failing_prompt_info.score:.2f}). Current population: {len(self.prompt_population)} prompts.")

        # Ensure the failing prompt is in the population (or updated if it was already there from a previous Evo run)
        # This also handles the initial population setup if this is the very first call for a task.
        self._add_to_population(failing_prompt_info) # _add_to_population handles sorting and pruning

        # The 'failing_prompt_info' is the one that triggered *this* specific call to run_self_healing_iteration.
        # It will be used as a parent for the first EvoPrompt internal iteration.
        prompt_to_evolve_from = failing_prompt_info

        for t in range(EVO_PROMPT_ITERATIONS_T):
            self.current_evo_iteration = t + 1 # t is 0-indexed, iterations are 1-indexed
            print(f"  [PromptModifier - {self.task_id}] EvoPrompt internal iteration {self.current_evo_iteration}/{EVO_PROMPT_ITERATIONS_T}")

            # 1. Select Parent(s) (Task 3.4)
            # For now, we use the 'prompt_to_evolve_from' which is initially the failing prompt,
            # and in subsequent EvoPrompt iterations, it could become the best new candidate from the previous Evo iteration.
            # The select_parents method can be made more sophisticated later if needed (e.g. to combine multiple parents)
            # Here, we pass the critic report associated with the prompt we are evolving FROM.
            parents_to_evolve = self.select_parents(failing_prompt_info_for_current_iteration=prompt_to_evolve_from)
            if not parents_to_evolve:
                print(f"    No parents selected for evolution. Skipping EvoPrompt iteration {self.current_evo_iteration}.")
                continue
            
            # We'll evolve based on the first parent, which is usually the most relevant (e.g., current failing one)
            primary_parent_for_evolution = parents_to_evolve[0]
            # The critic_feedback for this parent is crucial for targeted evolution.
            # Ensure primary_parent_for_evolution.critic_report is not None, it should have been set.
            critic_feedback_for_parent = primary_parent_for_evolution.critic_report
            if not critic_feedback_for_parent:
                print(f"    ERROR: Critic report missing for parent prompt: {primary_parent_for_evolution.prompt[:50]}...")
                # Potentially skip or use a generic failure if this happens. For now, we'll try to proceed
                # or simply log and the evolve_prompts might handle it or fail.
                # A robust system might need a fallback or ensure this never happens.
                # For now, let's assume it's present based on how failing_prompt_info is constructed.

            # 2. Evolve Prompt(s) (Task 3.5)
            # evolve_prompts expects a list of parent PromptInfo objects and the specific critic_report of the primary parent it's evolving from.
            candidate_prompt_strings = self.evolve_prompts(parents=[primary_parent_for_evolution], critic_feedback=critic_feedback_for_parent)
            
            if not candidate_prompt_strings:
                print(f"    LLM evolution did not yield any candidate prompts. Skipping further processing for this Evo iteration.")
                continue

            any_candidate_improved_score_in_this_evo_iter = False
            best_candidate_in_this_evo_iter_info = None

            for candidate_prompt_text in candidate_prompt_strings:
                print(f"    Evaluating candidate prompt: \"{candidate_prompt_text[:70]}...\"")
                
                # 3. Evaluate Candidate Prompt (Task 3.6 - via orchestrator_callback)
                try:
                    # The callback handles Executor -> Critic execution
                    critic_report_for_candidate = orchestrator_callback_evaluate_candidate(
                        candidate_prompt_text,
                        original_task_description # Pass this along if the executor/critic callback needs it
                    )
                except Exception as e:
                    print(f"      Error during orchestrator_callback_evaluate_candidate for prompt \"{candidate_prompt_text[:70]}...\": {e}")
                    # Create a dummy critic report indicating evaluation failure
                    from self_healing_agents.schemas import CRITIC_STATUS_FAILURE_EVALUATION # Assuming this status exists
                    critic_report_for_candidate = CriticReport(
                        status=CRITIC_STATUS_FAILURE_EVALUATION if 'CRITIC_STATUS_FAILURE_EVALUATION' in globals() else "FAILURE_EVALUATION",
                        score=0.0, # Penalize heavily
                        summary=f"Failed to evaluate candidate prompt due to orchestrator callback error: {e}",
                        error_details=str(e),
                        test_results=[]
                    )

                if not critic_report_for_candidate: # Should not happen if callback is robust
                    print(f"      Orchestrator callback returned None for candidate prompt. Skipping.")
                    continue

                evaluated_candidate_info = self.evaluate_candidate_prompt(
                    candidate_prompt=candidate_prompt_text,
                    original_task_description=original_task_description, # Not directly used by evaluate_candidate_prompt but good for consistency
                    current_critic_report_for_candidate=critic_report_for_candidate
                )
                
                # 4. Update Population (Task 3.7)
                self._add_to_population(evaluated_candidate_info)
                print(f"      Candidate evaluated. Score: {evaluated_candidate_info.score:.2f}. Added to population (new size: {len(self.prompt_population)}).")

                if evaluated_candidate_info.score > primary_parent_for_evolution.score:
                    any_candidate_improved_score_in_this_evo_iter = True
                
                if best_candidate_in_this_evo_iter_info is None or evaluated_candidate_info.score > best_candidate_in_this_evo_iter_info.score:
                    best_candidate_in_this_evo_iter_info = evaluated_candidate_info

            # After evaluating all candidates from this evolution step,
            # update 'prompt_to_evolve_from' for the *next* EvoPrompt iteration (if T > 1).
            # We want the next Evo iteration to build upon the best thing found *in this current Evo iteration*.
            if best_candidate_in_this_evo_iter_info and best_candidate_in_this_evo_iter_info.score > prompt_to_evolve_from.score:
                print(f"    Best candidate in Evo iteration {self.current_evo_iteration} (Score: {best_candidate_in_this_evo_iter_info.score:.2f}) is better than previous parent (Score: {prompt_to_evolve_from.score:.2f}). Updating base for next Evo iteration.")
                prompt_to_evolve_from = best_candidate_in_this_evo_iter_info
            elif any_candidate_improved_score_in_this_evo_iter: # A candidate was better than parent, but not overall best_candidate
                 # This case is tricky, if best_candidate_in_this_evo_iter_info wasn't better than prompt_to_evolve_from (which could be a historic best),
                 # but *some* candidate was better than its direct parent.
                 # For simplicity, we'll stick to evolving from the best discovered so far in the Evo run or the initial failing prompt.
                 pass


        # 5. Return Best (Task 3.8)
        if not self.prompt_population:
            print(f"  [PromptModifier - {self.task_id}] Population is empty after {EVO_PROMPT_ITERATIONS_T} EvoPrompt iterations. No prompt to return.")
            return None

        best_prompt_overall = self.prompt_population[0] # Population is sorted
        
        # Log the best prompt from the population before returning
        print(f"  [PromptModifier - {self.task_id}] Finished {EVO_PROMPT_ITERATIONS_T} EvoPrompt iterations. Best prompt in population (Score: {best_prompt_overall.score:.2f}): \"{best_prompt_overall.prompt[:70]}...\"")
        
        # As per PRD: "After T iterations ... the prompt with the highest score ... is selected and returned to update the Executor's prompt"
        # Always return the best prompt found in the population after the EvoPrompt cycle.
        # The main harness will then use this to update the Executor.
        # We also compare to the initial failing prompt for logging/decision-making if needed, but the return is the best found.

        if best_prompt_overall.score > failing_prompt_info.score:
            print(f"    Selected best prompt from population (Score: {best_prompt_overall.score:.2f}) is an improvement over the prompt that triggered this healing cycle (Score: {failing_prompt_info.score:.2f}).")
        elif best_prompt_overall.score == failing_prompt_info.score:
            print(f"    Selected best prompt from population (Score: {best_prompt_overall.score:.2f}) has the same score as the prompt that triggered this healing cycle (Score: {failing_prompt_info.score:.2f}).")
        else:
            print(f"    Selected best prompt from population (Score: {best_prompt_overall.score:.2f}) has a lower score than the prompt that triggered this healing cycle (Score: {failing_prompt_info.score:.2f}).")

        print(f"    Returning this best prompt from population to the main harness.")
        return best_prompt_overall.prompt


# Placeholder for Orchestrator if PromptModifier needs to call it directly
# (Not recommended, prefer callbacks)
# class OrchestratorPlaceholder:
    # ... existing code ...

    # ... rest of the existing methods ... 