"""
Failure Classifier for Multi-Agent Self-Healing System

This module classifies failures as either planning-related or execution-related
to enable targeted self-healing of the appropriate agent.
"""

from enum import Enum
from typing import Dict, Any, List, Optional, Set
import re
import logging

logger = logging.getLogger(__name__)

class FailureType(Enum):
    """Types of failures that can occur in the multi-agent system."""
    PLANNING_FAILURE = "PLANNING_FAILURE"
    EXECUTION_FAILURE = "EXECUTION_FAILURE"
    MIXED_FAILURE = "MIXED_FAILURE"  # Both planning and execution issues
    UNKNOWN_FAILURE = "UNKNOWN_FAILURE"

class FailureClassifier:
    """
    Analyzes errors and context to classify failures as planning vs execution issues.
    """
    
    def __init__(self):
        self.planning_failure_indicators = {
            # Logic and flow issues
            "missing_steps", "incomplete_plan", "logical_inconsistency", 
            "circular_dependency", "undefined_variable", "variable_used_before_definition",
            "missing_import_in_plan", "invalid_sequence", "infeasible_operation",
            
            # Abstract/incomplete guidance
            "abstract_step", "vague_instruction", "missing_specification",
            "unclear_requirement", "ambiguous_step",
            
            # Resource/dependency issues in planning
            "missing_dependency", "resource_not_specified", "environment_assumption",
            "tool_availability", "api_endpoint_missing"
        }
        
        self.execution_failure_indicators = {
            # Syntax and import errors
            "syntax_error", "import_error", "module_not_found", "name_error",
            "attribute_error", "type_error", "indentation_error", "invalid_syntax",
            
            # Runtime errors during valid operations  
            "runtime_error", "value_error", "key_error", "index_error", 
            "zero_division", "file_not_found", "permission_error",
            
            # Implementation bugs
            "off_by_one", "wrong_condition", "incorrect_loop", "bad_algorithm",
            "edge_case_not_handled", "incorrect_data_structure"
        }
        
    def classify_failure(
        self, 
        plan: Dict[str, Any], 
        code: str, 
        error_report: Dict[str, Any],
        task_description: str,
        execution_context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Classify a failure as planning or execution related.
        
        Args:
            plan: The plan generated by the planner
            code: The code generated by the executor  
            error_report: Error details from the critic
            task_description: Original task description
            execution_context: Additional context about execution environment
            
        Returns:
            Dict containing classification results and reasoning
        """
        logger.info("ðŸ” FAILURE CLASSIFICATION: Starting failure analysis...")
        
        classification_result = {
            "failure_type": FailureType.UNKNOWN_FAILURE,
            "confidence": 0.0,
            "reasoning": [],
            "planning_issues": [],
            "execution_issues": [],
            "recommended_healing_target": "EXECUTOR",  # Default
            "suggested_improvements": []
        }
        
        planning_score = 0.0
        execution_score = 0.0
        
        # Analyze different aspects of the failure
        planning_score += self._analyze_plan_quality(plan, error_report, classification_result)
        planning_score += self._analyze_logical_consistency(plan, code, error_report, classification_result)
        execution_score += self._analyze_syntax_runtime_errors(error_report, classification_result)
        execution_score += self._analyze_implementation_quality(code, error_report, classification_result)
        execution_score += self._analyze_test_failures(error_report, classification_result)
        
        # Special case: Check for plan-code mismatch
        mismatch_score = self._analyze_plan_code_mismatch(plan, code, error_report, classification_result)
        if mismatch_score > 0.7:
            planning_score += mismatch_score
            
        # Normalize scores  
        total_score = planning_score + execution_score
        if total_score > 0:
            planning_ratio = planning_score / total_score
            execution_ratio = execution_score / total_score
        else:
            planning_ratio = execution_ratio = 0.5
            
        # Determine failure type and confidence
        confidence_threshold = 0.65
        
        if planning_ratio > confidence_threshold:
            classification_result["failure_type"] = FailureType.PLANNING_FAILURE
            classification_result["confidence"] = planning_ratio
            classification_result["recommended_healing_target"] = "PLANNER"
        elif execution_ratio > confidence_threshold:
            classification_result["failure_type"] = FailureType.EXECUTION_FAILURE  
            classification_result["confidence"] = execution_ratio
            classification_result["recommended_healing_target"] = "EXECUTOR"
        elif abs(planning_ratio - execution_ratio) < 0.2:
            classification_result["failure_type"] = FailureType.MIXED_FAILURE
            classification_result["confidence"] = 0.5
            classification_result["recommended_healing_target"] = "PLANNER"  # Start with planning
        else:
            classification_result["failure_type"] = FailureType.UNKNOWN_FAILURE
            classification_result["confidence"] = max(planning_ratio, execution_ratio)
            classification_result["recommended_healing_target"] = "EXECUTOR"  # Default fallback
            
        # Add summary reasoning
        classification_result["reasoning"].append(
            f"Planning score: {planning_score:.2f}, Execution score: {execution_score:.2f}"
        )
        classification_result["reasoning"].append(
            f"Planning ratio: {planning_ratio:.2f}, Execution ratio: {execution_ratio:.2f}"
        )
        
        logger.info(f"ðŸŽ¯ FAILURE CLASSIFICATION: {classification_result['failure_type'].value}")
        logger.info(f"ðŸŽ¯ CONFIDENCE: {classification_result['confidence']:.2f}")
        logger.info(f"ðŸŽ¯ RECOMMENDED TARGET: {classification_result['recommended_healing_target']}")
        
        return classification_result
        
    def _analyze_plan_quality(self, plan: Dict[str, Any], error_report: Dict[str, Any], result: Dict[str, Any]) -> float:
        """Analyze the quality and completeness of the plan."""
        score = 0.0
        
        if not isinstance(plan, dict):
            result["planning_issues"].append("Plan is not in expected dictionary format")
            return 0.8
            
        # Check for plan structure
        plan_steps = plan.get("steps", plan.get("plan_steps", []))
        if not plan_steps:
            result["planning_issues"].append("No execution steps found in plan")
            score += 0.6
            
        # Check for abstract/vague steps
        if isinstance(plan_steps, list):
            for step in plan_steps:
                step_text = str(step).lower()
                if any(vague in step_text for vague in ["implement", "create", "handle", "process"] 
                       if len(step_text.split()) < 5):
                    result["planning_issues"].append(f"Vague step found: {step}")
                    score += 0.3
                    
        # Check for missing critical information
        if "requirements" not in plan and "constraints" not in plan:
            result["planning_issues"].append("No requirements or constraints specified")
            score += 0.2
            
        return min(score, 1.0)
        
    def _analyze_logical_consistency(self, plan: Dict[str, Any], code: str, error_report: Dict[str, Any], result: Dict[str, Any]) -> float:
        """Check for logical inconsistencies between plan and requirements."""
        score = 0.0
        
        # Check for undefined variable errors that suggest planning issues
        error_message = str(error_report.get("execution_error_message", "")).lower()
        if "name" in error_message and "not defined" in error_message:
            # Extract variable name
            match = re.search(r"name '(\w+)' is not defined", error_message)
            if match:
                var_name = match.group(1)
                plan_text = str(plan).lower()
                if var_name not in plan_text and var_name not in ["print", "len", "range", "enumerate"]:
                    result["planning_issues"].append(f"Variable '{var_name}' used but not planned")
                    score += 0.7
                    
        # Check for import errors that should have been planned
        if "import" in error_message and ("module" in error_message or "package" in error_message):
            result["planning_issues"].append("Missing import specification in plan")
            score += 0.5
            
        return min(score, 1.0)
        
    def _analyze_syntax_runtime_errors(self, error_report: Dict[str, Any], result: Dict[str, Any]) -> float:
        """Analyze syntax and runtime errors that indicate execution issues."""
        score = 0.0
        
        error_type = error_report.get("execution_error_type", "").lower()
        error_message = str(error_report.get("execution_error_message", "")).lower()
        
        # Syntax errors are clearly execution issues
        syntax_indicators = ["syntaxerror", "indentationerror", "invalid syntax"]
        if any(indicator in error_type for indicator in syntax_indicators):
            result["execution_issues"].append(f"Syntax error: {error_type}")
            score += 0.9
            
        # Runtime errors during implementation
        runtime_indicators = ["typeerror", "valueerror", "attributeerror", "keyerror", "indexerror"]
        if any(indicator in error_type for indicator in runtime_indicators):
            result["execution_issues"].append(f"Runtime error: {error_type}")
            score += 0.7
            
        # Specific error patterns
        if "unexpected indent" in error_message or "unindent" in error_message:
            result["execution_issues"].append("Indentation error in code implementation")
            score += 0.8
            
        return min(score, 1.0)
        
    def _analyze_implementation_quality(self, code: str, error_report: Dict[str, Any], result: Dict[str, Any]) -> float:
        """Analyze code implementation quality issues."""
        score = 0.0
        
        if not code or len(code.strip()) == 0:
            result["execution_issues"].append("No code was generated")
            return 0.8
            
        # Check for incomplete implementations
        if "pass" in code and len(code.split('\n')) < 5:
            result["execution_issues"].append("Incomplete implementation with pass statements")
            score += 0.6
            
        # Check for obvious syntax issues in code
        if code.count('(') != code.count(')'):
            result["execution_issues"].append("Unmatched parentheses in code")
            score += 0.7
            
        if code.count('[') != code.count(']'):
            result["execution_issues"].append("Unmatched brackets in code")
            score += 0.7
            
        # Check for bad variable naming that suggests rushed implementation
        lines = code.split('\n')
        for line in lines:
            if re.search(r'\b[a-z]\b(?!\s*=)', line):  # Single letter variables (not assignments)
                result["execution_issues"].append("Poor variable naming suggests hasty implementation")
                score += 0.3
                break
                
        return min(score, 1.0)
        
    def _analyze_test_failures(self, error_report: Dict[str, Any], result: Dict[str, Any]) -> float:
        """Analyze test failure patterns to determine if they're implementation bugs."""
        score = 0.0
        
        failed_tests = error_report.get("failed_test_details", [])
        num_failed = error_report.get("num_tests_failed", 0)
        
        if num_failed == 0:
            return 0.0
            
        # Analyze failure patterns
        for test in failed_tests:
            error_msg = str(test.get("error_message", "")).lower()
            
            # Assertion errors suggest logical implementation issues
            if "assertionerror" in error_msg or "assertion" in error_msg:
                result["execution_issues"].append("Test assertion failures indicate implementation bugs")
                score += 0.6
                
            # Wrong output type suggests implementation error
            expected = test.get("expected_output_spec", "")
            actual = test.get("actual_output", "")
            if expected and actual and type(expected) != type(actual):
                result["execution_issues"].append("Output type mismatch indicates implementation error")
                score += 0.5
                
        return min(score, 1.0)
        
    def _analyze_plan_code_mismatch(self, plan: Dict[str, Any], code: str, error_report: Dict[str, Any], result: Dict[str, Any]) -> float:
        """Check if the code doesn't follow the plan, indicating plan inadequacy."""
        score = 0.0
        
        plan_steps = plan.get("steps", plan.get("plan_steps", []))
        if not plan_steps or not code:
            return 0.0
            
        # Check if plan mentions specific functions/variables that don't appear in code
        plan_text = str(plan).lower()
        code_lower = code.lower()
        
        # Extract function names from plan
        function_mentions = re.findall(r'function\s+(\w+)|def\s+(\w+)|(\w+)\s*\(', plan_text)
        planned_functions = [f for group in function_mentions for f in group if f]
        
        missing_functions = []
        for func in planned_functions:
            if func not in code_lower and len(func) > 2:  # Avoid short common words
                missing_functions.append(func)
                
        if missing_functions:
            result["planning_issues"].append(f"Planned functions not implemented: {missing_functions}")
            score += 0.4
            
        # Check if plan mentions specific algorithms/approaches not in code
        algorithm_keywords = ["sort", "binary search", "hash", "recursion", "iteration", "loop"]
        for keyword in algorithm_keywords:
            if keyword in plan_text and keyword not in code_lower:
                result["planning_issues"].append(f"Planned approach '{keyword}' not implemented")
                score += 0.2
                
        return min(score, 1.0) 