"""
LLM-Based Failure Classifier for Multi-Agent Self-Healing System

This module uses an LLM to intelligently classify failures and determine which
agent should be targeted for healing based on test results and context analysis.
"""

import json
import logging
from typing import Dict, Any, List, Optional
from enum import Enum

logger = logging.getLogger(__name__)

class FailureType(Enum):
    """Types of failures that can occur in the multi-agent system."""
    PLANNING_FAILURE = "PLANNING_FAILURE"
    EXECUTION_FAILURE = "EXECUTION_FAILURE"
    CRITIC_FAILURE = "CRITIC_FAILURE"
    DATA_PROCESSING_FAILURE = "DATA_PROCESSING_FAILURE"
    UNKNOWN_FAILURE = "UNKNOWN_FAILURE"

class LLMFailureClassifier:
    """
    Uses LLM reasoning to classify failures and determine healing targets.
    Scalable to multiple agents and complex failure scenarios.
    """
    
    def __init__(self, llm_service):
        self.llm_service = llm_service
        self.classification_prompt = self._build_classification_prompt()
        
    def _build_classification_prompt(self) -> str:
        """Build the LLM prompt for failure classification."""
        return """You are an expert multi-agent system failure analyst. Your job is to analyze failures and determine which agent(s) are responsible and should be targeted for healing.

You will be given:
1. The original task description
2. The plan generated by the Planner agent
3. The code generated by the Executor agent  
4. Test results and error details from the Critic agent
5. Any additional context

Your goal is to determine:
- Which agent(s) are primarily responsible for the failure
- The confidence level of your assessment
- Detailed reasoning for your decision
- Specific healing recommendations

IMPORTANT: Respond with a JSON object in this exact format:
{
  "primary_failure_type": "PLANNING_FAILURE|EXECUTION_FAILURE|CRITIC_FAILURE|DATA_PROCESSING_FAILURE|UNKNOWN_FAILURE",
  "recommended_healing_target": "PLANNER|EXECUTOR",
  "confidence": 0.85,
  "reasoning": [
    "Detailed reason 1",
    "Detailed reason 2"
  ],
  "specific_issues": {
    "planning_issues": ["Issue 1", "Issue 2"],
    "execution_issues": ["Issue 1", "Issue 2"],
    "critic_issues": ["Issue 1"],
    "other_issues": ["Issue 1"]
  },
  "healing_recommendations": [
    "Specific recommendation 1",
    "Specific recommendation 2"
  ],
  "failure_severity": "LOW|MEDIUM|HIGH|CRITICAL"
}

ANALYSIS GUIDELINES:

**PLANNING_FAILURE indicators:**
- Plan is too vague, abstract, or incomplete
- Plan doesn't match the task requirements
- Plan has logical inconsistencies or impossible steps
- Plan lacks critical details needed for implementation
- Plan suggests wrong algorithms or approaches

**EXECUTION_FAILURE indicators:**
- Code has syntax errors, runtime errors, or bugs
- Code doesn't follow the plan correctly
- Code has implementation flaws or logical errors
- Code fails test cases due to incorrect logic
- Code is incomplete or poorly structured

**DECISION REQUIREMENT:**
You MUST choose either PLANNING_FAILURE or EXECUTION_FAILURE as the primary failure type.
Even if both planning and execution have issues, determine which is the ROOT CAUSE:
- If the plan is fundamentally inadequate (vague, missing steps, wrong approach), choose PLANNING_FAILURE
- If the plan is reasonable but the code implementation is flawed, choose EXECUTION_FAILURE
- When in doubt, consider: "Would a better plan have prevented this failure?" If yes, choose PLANNING_FAILURE

**CRITIC_FAILURE indicators:**
- Test cases are inappropriate or insufficient
- Evaluation criteria don't match task requirements
- Test execution errors unrelated to code quality

Consider the TEST CASE RESULTS carefully:
- What types of tests are failing?
- Are failures due to wrong logic, syntax errors, or missing functionality?
- Do error messages point to specific implementation issues?
- Are the test cases themselves reasonable and comprehensive?

CRITICAL: You MUST make a binary choice between PLANNER and EXECUTOR healing.
- Choose PLANNER if the root cause is inadequate planning (vague steps, missing approach, wrong strategy)
- Choose EXECUTOR if the root cause is implementation issues (syntax errors, logic bugs, runtime failures)
- When both have issues, choose the PRIMARY ROOT CAUSE that would prevent success

Be specific and evidence-based in your reasoning."""

    def classify_failure(
        self,
        task_description: str,
        plan: Dict[str, Any],
        code: str,
        error_report: Dict[str, Any],
        additional_context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Use LLM to classify the failure and determine healing target.
        
        Args:
            task_description: Original task description
            plan: Plan generated by Planner agent
            code: Code generated by Executor agent
            error_report: Error report from Critic agent
            additional_context: Additional context for analysis
            
        Returns:
            Dict containing classification results and recommendations
        """
        logger.info("ðŸ¤– LLM FAILURE CLASSIFICATION: Starting LLM-based failure analysis...")
        
        # Extract test case details for focused analysis
        test_results = self._extract_test_details(error_report)
        
        # Build analysis prompt with all context
        analysis_prompt = self._build_analysis_prompt(
            task_description, plan, code, error_report, test_results, additional_context
        )
        
        try:
            # Get LLM classification
            messages = [
                {"role": "system", "content": self.classification_prompt},
                {"role": "user", "content": analysis_prompt}
            ]
            
            response = self.llm_service.invoke(messages, expect_json=True)
            
            # Validate and process response
            classification = self._validate_classification_response(response)
            
            # Log results
            self._log_classification_results(classification)
            
            return classification
            
        except Exception as e:
            logger.error(f"ðŸš¨ LLM FAILURE CLASSIFICATION ERROR: {e}")
            return self._fallback_classification(error_report)
    
    def _extract_test_details(self, error_report: Dict[str, Any]) -> Dict[str, Any]:
        """Extract detailed test case information for analysis."""
        test_details = {
            "total_tests": 0,
            "passed_tests": 0,
            "failed_tests": 0,
            "error_types": [],
            "failure_patterns": [],
            "sample_failures": []
        }
        
        # Extract from error report structure
        test_results = error_report.get("test_results", [])
        if test_results:
            test_details["total_tests"] = len(test_results)
            
            for test in test_results:
                if test.get("status") == "passed" or test.get("passed", False):
                    test_details["passed_tests"] += 1
                else:
                    test_details["failed_tests"] += 1
                    
                    # Collect error information
                    error_msg = test.get("error_message", "")
                    if error_msg:
                        test_details["error_types"].append(error_msg)
                    
                    # Sample detailed failure for analysis
                    if len(test_details["sample_failures"]) < 3:
                        test_details["sample_failures"].append({
                            "test_name": test.get("name", test.get("test_case_name", "unknown")),
                            "inputs": test.get("inputs", {}),
                            "expected": test.get("expected_output_spec", test.get("expected", "unknown")),
                            "actual": test.get("actual_output", test.get("actual", "unknown")),
                            "error": error_msg
                        })
        
        return test_details
    
    def _build_analysis_prompt(
        self,
        task_description: str,
        plan: Dict[str, Any],
        code: str,
        error_report: Dict[str, Any],
        test_results: Dict[str, Any],
        additional_context: Optional[Dict[str, Any]]
    ) -> str:
        """Build the specific analysis prompt with all context."""
        
        prompt = f"""
FAILURE ANALYSIS REQUEST

**ORIGINAL TASK:**
{task_description}

**PLANNER OUTPUT:**
{json.dumps(plan, indent=2)}

**EXECUTOR OUTPUT (CODE):**
```python
{code}
```

**TEST RESULTS SUMMARY:**
- Total Tests: {test_results['total_tests']}
- Passed: {test_results['passed_tests']}
- Failed: {test_results['failed_tests']}
- Success Rate: {(test_results['passed_tests'] / max(test_results['total_tests'], 1)) * 100:.1f}%

**DETAILED FAILURE ANALYSIS:**
"""
        
        # Add sample failure details
        if test_results["sample_failures"]:
            prompt += "\n**SAMPLE TEST FAILURES:**\n"
            for i, failure in enumerate(test_results["sample_failures"], 1):
                prompt += f"""
Test {i}: {failure['test_name']}
- Inputs: {failure['inputs']}
- Expected: {failure['expected']}
- Actual: {failure['actual']}
- Error: {failure['error']}
"""
        
        # Add error report summary
        if error_report.get("summary"):
            prompt += f"\n**CRITIC SUMMARY:**\n{error_report['summary']}\n"
        
        # Add execution details if available
        if error_report.get("execution_stdout") or error_report.get("execution_stderr"):
            prompt += f"""
**EXECUTION DETAILS:**
- Stdout: {error_report.get('execution_stdout', 'None')[:200]}...
- Stderr: {error_report.get('execution_stderr', 'None')[:200]}...
"""
        
        # Add additional context
        if additional_context:
            prompt += f"\n**ADDITIONAL CONTEXT:**\n{json.dumps(additional_context, indent=2)}\n"
        
        prompt += """
**YOUR TASK:**
Analyze the above information and determine:
1. Which agent is primarily responsible for the failure?
2. What specific issues caused the failure?
3. How confident are you in this assessment?
4. What specific healing actions should be taken?

Focus especially on the test case failures and error patterns to guide your decision.
"""
        
        return prompt
    
    def _validate_classification_response(self, response: Any) -> Dict[str, Any]:
        """Validate and clean the LLM classification response."""
        if not isinstance(response, dict):
            raise ValueError(f"Expected dict response, got {type(response)}")
        
        required_fields = [
            "primary_failure_type", "recommended_healing_target", 
            "confidence", "reasoning"
        ]
        
        for field in required_fields:
            if field not in response:
                raise ValueError(f"Missing required field: {field}")
        
        # Validate enums
        valid_failure_types = [ft.value for ft in FailureType]
        if response["primary_failure_type"] not in valid_failure_types:
            response["primary_failure_type"] = "EXECUTION_FAILURE"  # Default to execution
        
        # Force binary choice - only PLANNER or EXECUTOR allowed
        valid_targets = ["PLANNER", "EXECUTOR"]
        if response["recommended_healing_target"] not in valid_targets:
            # Default to EXECUTOR for unknown cases
            response["recommended_healing_target"] = "EXECUTOR"
            
        # Auto-map failure type to healing target if consistent
        if response["primary_failure_type"] == "PLANNING_FAILURE":
            response["recommended_healing_target"] = "PLANNER"
        elif response["primary_failure_type"] == "EXECUTION_FAILURE":
            response["recommended_healing_target"] = "EXECUTOR"
        
        # Validate confidence
        confidence = response.get("confidence", 0.5)
        if not isinstance(confidence, (int, float)) or confidence < 0 or confidence > 1:
            response["confidence"] = 0.5
        
        # Ensure reasoning is a list
        if not isinstance(response.get("reasoning", []), list):
            response["reasoning"] = [str(response.get("reasoning", "No reasoning provided"))]
        
        return response
    
    def _fallback_classification(self, error_report: Dict[str, Any]) -> Dict[str, Any]:
        """Provide fallback classification if LLM fails."""
        logger.warning("Using fallback classification due to LLM error")
        
        return {
            "primary_failure_type": "EXECUTION_FAILURE",
            "recommended_healing_target": "EXECUTOR",
            "confidence": 0.3,
            "reasoning": ["LLM classification failed, using fallback"],
            "specific_issues": {
                "planning_issues": [],
                "execution_issues": ["LLM analysis unavailable"],
                "critic_issues": [],
                "other_issues": []
            },
            "healing_recommendations": ["Retry with basic healing approach"],
            "failure_severity": "MEDIUM"
        }
    
    def _log_classification_results(self, classification: Dict[str, Any]) -> None:
        """Log the classification results."""
        logger.info(f"ðŸŽ¯ LLM CLASSIFICATION: {classification['primary_failure_type']}")
        logger.info(f"ðŸŽ¯ CONFIDENCE: {classification['confidence']:.2f}")
        logger.info(f"ðŸŽ¯ RECOMMENDED TARGET: {classification['recommended_healing_target']}")
        
        if classification.get("reasoning"):
            logger.info("ðŸŽ¯ REASONING:")
            for reason in classification["reasoning"]:
                logger.info(f"   - {reason}") 